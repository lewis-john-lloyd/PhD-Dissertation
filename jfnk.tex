\group{Jacobian-Free Newton Krylov Overview}
In the Jacobian-Free Newton Krylov framework, the following procedure is taken:

%%---------------------------------------------------------------------------------------------------
%%            Discretization of Governing Equations
%%---------------------------------------------------------------------------------------------------
%
%\subgroup{Discretization of Governing Equations}
%The governing partial differential equations all have the following general form:
%
%\begin{equation}
%\DerivParOne{\Vec{u}}{t} = \mathcal{F}\left(\Vec{u},\;\DerivParOne{\Vec{u}}{\Vec{x}},\;\DerivParOne{^2\Vec{u}}{\Vec{x}^2}\right)
%\end{equation}
%
%The time derivative of the unknown vector function, \Vec{u}, is a functional of both itself and its derivatives.
%In practice, \Vec{u} is not solved for directly.
%Instead, \Vec{u} is parametrized by a different set of unknowns, \Vec{q}.
%By using either a discrete approximation to both the temporal and the spatial derivatives of \Vec{u}, a discretized form of the governing equations is obtained. 
%
%\begin{equation}
%\Vec{T}(\Vec{u}(\Vec{q})) = \Vec{S}(\Vec{u}(\Vec{q}))
%\end{equation}
%
%Where \Vec{T} and \Vec{S} represent the discrete temporal and the spatial approximation schemes, respectively.
%Any constants in the PDEs are group with \Vec{S}.

%---------------------------------------------------------------------------------------------------
%            Nonlinear Function
%---------------------------------------------------------------------------------------------------
\begin{algorithm}
\setlength{\baselineskip}{0.625\baselineskip}
\label{TransientLoop}
\caption{Transient Loop}
\begin{algorithmic}[1]
\Require $\Vec{x}^{0}$ and $t^{0}$
\Set $n = 0$
\Loop \; Take a Time Step
    \Set $\vec{x}^{n}$        
    \Calculate $\Delta t$ 
    \State $t^{n+1} : = t^{n} + \Delta t$
    \BlackBox Solve for $\vec{x}^{n+1}$ 
    \Test CCFL \Comment{Time-step Failure Mechanism (ccfl\_fail) }
    \BlackBox Interfacial Area Transport Equation
    \Calculate Courant Numbers 
\EndLoop{\;$n = n+1$}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\setlength{\baselineskip}{0.625\baselineskip}
\label{ModifiedNewtonsAlgorithm}
\caption{Modified Newton's Method - Frozen Jacobian}
\begin{algorithmic}[1]
\Define $\vec{x}^{n+1}_0$ 
\Calculate Junction Values
\Calculate $\vec{u}^{n+1}_0$
\Set $\vec{u}^{*}_0 = \vec{u}^{n+1}_0$
\State Apply BCs
\Set $k = 0$
\Loop \quad Take a Newton Step
    \If{ $k = 0$}
        \Calculate $\mathbf{J}_{0} \equiv \Mat{J}(\Vec{x}^{n+1}_0)$
        \Calculate $\mathbf{E} \equiv \mathbf{E}(\Vec{x}^{n})$
    \Else
        \State Apply BCs
    \EndIf
    \Calculate $\mathbf{I}_k \equiv \Vec{I}(\Vec{x}^{n+1}_k)$
    \Calculate $\mathbf{F}_{k} = \mathbf{E} + \mathbf{I}_k$
    \BlackBox Scale $\mathbf{F}_k$ and $\mathbf{J}_0$.
    \BlackBox $\Vec{\delta x}_k = -\mathbf{J}^{-1}_0\cdot \mathbf{F}_k$ \Comment{Solver Choice}
    \BlackBox $\Vec{x}^{n+1}_{k+1}$ \Comment{Line-search or Trust Region}
    \Update Material Properties \Comment{UpdateVariables}
    \Test Material Properties
    \Update Junction Values \Comment{UpdateJunctions}
    \Update $\vec{u}^{n+1}_{k+1}$ \Comment{CalcAxialVelocity and CalcTransVelocity}
    \Update $\Vec{u}^{*}_{k+1}$ \Comment{CalcModAxialVelocity and CalcModTransVelocity}
    \Calculate Convergence Norms
    \If{ Converged}
        \State \bf{exit loop}
    \EndIf
\EndLoop \quad $k = k+1$
\end{algorithmic}
\end{algorithm}


\subgroup{Non-linear Function}

The vector function, \Vec{F}(\vec{x}), is the nonlinear residual of the discrete version of the governing PDEs.
For the physics of interest in this work, \Vec{F}(\Vec{x}) is a non-linear function.
The degree of non-linearity of \Vec{F}(\Vec{x}) depends upon the choice of numerical method used to distretize the governing PDEs.
%Since the independent parameters of \Vec{F} are the aforementioned $\Vec{x}_{n+1}$, the subscript $_{n+1}$ will be dropped.
For implementational reasons, the non-linear function is broken into two parts: the explicit and the implicit portions.
As shown in equation \eqref{ImplicitAndExplicit}, the explicit component, \Vec{E}, is independent of \Vec{x}, while the implicit component, \Vec{I}(\Vec{x}), is a function of \Vec{x}.

\begin{equation}
\label{ImplicitAndExplicit}
\Vec{F}(\Vec{x}) = \Vec{E} + \Vec{I}(\Vec{x})
\end{equation}

Solving the non-linear function depends upon finding a \vec{x} such that equation \eqref{NonlinearFunction} is satisfied.

\begin{equation}
\label{NonlinearFunction}
\Vec{F}(\Vec{x}) = 0
\end{equation}

%---------------------------------------------------------------------------------------------------
%            Newton's Method
%---------------------------------------------------------------------------------------------------

\subgroup{Newton's Method}
Newton's Method for solving the nonlinear system, Equation \eqref{NonlinearFunction}, is an iterative process involving a multidimensional Taylor Series expansion.
A first order Taylor series expansion of \Vec{F}(\Vec{x}) near $\Vec{x}_0$ is shown in equation \eqref{TaylorSeriesExpansion}.

\begin{equation}
\label{TaylorSeriesExpansion}
\Vec{F}(\Vec{x}) = \Vec{F}(\Vec{x}_0+\Vec{\delta x})= \Vec{F}(\Vec{x}_0)+\Mat{J}(\Vec{x}_0)\cdot\Vec{\delta x}+\mathcal{O}(\Vec{\delta x}^2)
\end{equation}

Let $\Vec{x}_n$ represent a vector of known values of independent parameters, with $\Vec{x}_0$ being the initial conditions of the problem.
Let $\Vec{\delta x}_n$ be the changes in $\Vec{x}_n$ that will bring you from $\Vec{x}_n$ to $\Vec{x}_{n+1}$, where $\Vec{x}_{n+1}$ is the future time value of independent parameters.

\begin{equation}
\label{DeltaX}
\Vec{x}_{n+1} = \Vec{x}_n + \Vec{\delta x}_n
\end{equation}

Note that \Mat{J}($\Vec{x}_0$) is the Jacobian of \Vec{F}(\Vec{x}) evaluated at $\Vec{x}_0$. 
Using \eqref{TaylorSeriesExpansion}, equation \eqref{NonlinearFunction} can be recast as a linear system where the unknown is \Vec{\delta x}.

\begin{equation}
\label{NewtonsMethod}
\Mat{J}(\Vec{x})\cdot\Vec{\delta x} = -\Vec{F}(\Vec{x})
\end{equation}

Solving for \Vec{\delta x} in \eqref{NewtonsMethod} constitutes one Newton step.

\begin{algorithm}
\setlength{\baselineskip}{0.625\baselineskip}
\caption{Newton's Method}
\begin{algorithmic}[1]
\Define $x_0$
\Define tolerance
\State $\gamma := 1$
\State $k := 0$
\While{$ \gamma \leq \text{tolerance}$}\Comment{ Test convergence of non-linear step.}
    \State $F^k := F(x^k)$ \Comment{Evaluate non-linear function at $x^k$.}
    \State $J^k := J(x^k)$ \Comment{Evaluate Jacobian matrix at $x^k$.}
    \State $\delta x^k := J^{-k} \cdot F^k$ \Comment{Solve for $\delta x$ by applying the inverse of $J^k$ to $F^k$.}
    \State $x^{k+1} := x^k + \delta x^k$ \Comment{Calculate $x^{k+1}$.}
    \State $\gamma := \text{min}\left(\frac{\norm{J \delta x}{2}}{\norm{F}{2}},\frac{\norm{\delta x}{2}}{\norm{x}{2}}\right)$ \Comment{Calculate convergence criteria.}
    \State $k := k + 1$ \Comment{Increment index.}
\EndWhile
\end{algorithmic}
\end{algorithm}
%---------------------------------------------------------------------------------------------------
%            Krylov Solver
%---------------------------------------------------------------------------------------------------

\subsubgroup{Krylov Solver}

Once \eqref{NewtonsMethod} is solved for $\Vec{\delta x}$, a new \Vec{x} is obtain by equation \eqref{DeltaX}. in order to update $\Vec{x}_k$ to $\Vec{x}_{k+1}$. 
To solve this linear system, a Krylov subspace method is used, in particular GMRES.
We will first drop the $_k$ and $_{k+1}$ subscripts because we will be using $_j$ and $_{j+1}$ to denote the Krylov (inner) iteration count.

First, an initial guess for $\Vec{\delta x}$ is chosen, $\Vec{\delta x}_0$.
In the literature, this initial guess is often zero for a transient simulation; the assertion is that $\Vec{\delta x}$ should be small within a time step.
Allowing for a nonzero initial guess, define the residual:

\begin{equation}
\Vec{r}_0 = -\Vec{F}(\Vec{x}_k)-\Mat{J}(\Vec{x}_k)\cdot\Vec{\delta x}_0
\end{equation}

We now normalize the residual, $\displaystyle \Vec{v}_1 = \frac{\Vec{r}_0}{\norm{\Vec{r}_0}{2}}$.
This vector will serve as our first basis vector (we need to start somewhere).
We then enter into an iterative process starting with j = 1.
Now compute the Jacobian-vector product, $\Mat{J}(\Vec{x}_k)\cdot\Vec{v}_j$.
We do this using a variant of Equation \eqref{TaylorSeriesExpansion}.

\begin{eqnarray}
\label{JacobianFreeExpansion}
\Vec{F}(\Vec{x}_k + \epsilon \Vec{v}_j) & = & \Vec{F}(\Vec{x}_k + \epsilon \; \Mat{J}(\Vec{x}_k) \cdot \Vec{v}_j + \mathcal{O}((\epsilon \Vec{v}_j)^2) \\
\label{JacobianFreeApproximation}
\Mat{J} (\Vec{x}_k) \cdot \Vec{v}_j & = & \frac{\Vec{F}(\Vec{x}_k + \epsilon \Vec{v}_j) - \Vec{F}(\Vec{x}_k)}{\epsilon}
\end{eqnarray}

It is apparent that each Jacobian-vector product requires the evaluation of \Vec{F}(\Vec{x}) at a slightly perturbed value from the base state. 
The epsilon is a small value that is one of the tweaks that can affect convergence performance.
However, traditionally this is near machine round-off.


The vector resulting from the Jacobian-Vector product, $\Vec{w}_j$, is then put through a Gram-Schmidt orthogonalization with all previous Krylov vectors $\Vec{v}_1,\;\Vec{v}_2,\;\dots,\;\Vec{v}_{j-1},\;\Vec{v}_j$.
There are alternative options for this point in the algorithm (such as the Householder variation of the Arnoldi process) - these are additional options that you can change in PETSc.
Part of this process (the inner-product with previous Kyrlov vectors) generates entries for the $j^{th}$ column of a matrix, \textbf{H}.
The projections of each previous Krylov vector onto $\Vec{w}_j$ are subtracted from $\Vec{w}_j$ (this is Gram-Schmidt). 
This new Krylov vector is determined by normalizing $\Vec{w}_j$, $\displaystyle \Vec{v}_{j+1} = \frac{\Vec{w}_j}{\norm{\Vec{w}_j}{2}}$.
\textbf{Side Note:} the norm of $\Vec{w}_j$ is the $h_{j+1,j}$ entry in the matrix $\mathbf{H}$.

The stopping criteria for this process is related to the norm of the latest residual, Equation \eqref{KrylovResidual}, and the norm of $-\Vec{F}(\Vec{x}_k)$.
The exact relation is determined by three tunable knobs in PETSc.
\begin{equation}
\label{KrylovResidual}
\norm{\Vec{r}_j}{2} = \norm{-\Vec{F}-\Vec{J}\cdot\Vec{\delta x}_j}{2}
\end{equation}
\textbf{NOTE:} The residual in Equation \eqref{KrylovResidual} is generated independently of constructing $\Vec{\delta x}_j$ (the final answer), which is only done at the end of the GMRES process.

After the process has stopped, a least square process is applied using $\Vec{H}$ to determine the coefficients for a linear combination of $\Vec{v}$ that will result in a $\Vec{\delta x}$ that minimizes $\norm{-\Vec{F}(\Vec{x}_k)-\Mat{J}(\Vec{x_k})\cdot\Vec{\delta x}}{2}$.
That concludes the Krlov (inner) iteration.

This $\Vec{\delta x}_k$ then used to computer $\Vec{x}_{k+1}$, which is then used in a line search (or trust region) globalization step.
If good enough (PETSc options), then accept $\Vec{\delta x}_k$ and $\Vec{x}_{k+1}$ as $\Vec{\delta x}_n$ and $\Vec{x}_{n+1}$.
This ends the Newton (outer) iteration.

Take next time step.


%---------------------------------------------------------------------------------------------------
%            Globalization Strategies
%---------------------------------------------------------------------------------------------------

\subsubgroup{Globalization Strategies}

\subsubsubgroup{Line Search}

\subsubsubgroup{Trust Region}
